{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: 機械学習\n",
    "本章では，Bo Pang氏とLillian Lee氏が公開しているMovie Review Dataのsentence polarity dataset v1.0を用い，文を肯定的（ポジティブ）もしくは否定的（ネガティブ）に分類するタスク（極性分析）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. データの入手・整形\n",
    "文に関する極性分析の正解データを用い，以下の要領で正解データ（sentiment.txt）を作成せよ．\n",
    "\n",
    "1. rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する（極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "1. rt-polarity.negの各行の先頭に\"-1 \"という文字列を追加する（極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "1. 上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "\n",
    "sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正例: 5331件\n",
      "負例: 5331件\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def knock_70():\n",
    "    with codecs.open('../data/rt-polaritydata/rt-polarity.pos', 'r', 'cp1252') as f_pos:\n",
    "        df_pos = pd.concat([pd.DataFrame([['+1', sentence.rstrip('\\n')]]) for sentence in f_pos], ignore_index=True)        \n",
    "    with codecs.open('../data/rt-polaritydata/rt-polarity.neg', 'r', 'cp1252') as f_neg:\n",
    "        df_neg = pd.concat([pd.DataFrame([['-1', sentence.rstrip('\\n')]]) for sentence in f_neg], ignore_index=True)\n",
    "    df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    df.columns = ['Sentiment', 'Review']\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "    df.to_csv('../work/sentiment.txt', sep=' ', index=False)\n",
    "\n",
    "    print('正例: ' + str((df['Sentiment'] == '+1').sum()) + '件') \n",
    "    print('負例: ' + str((df['Sentiment'] == '-1').sum()) + '件')\n",
    "\n",
    "knock_70()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Review\r\n",
      "-1 \"to portray modern women the way director davis has done is just unthinkable . \"\r\n",
      "-1 \"kenneth branagh's energetic sweet-and-sour performance as a curmudgeonly british playwright grounds this overstuffed , erratic dramedy in which he and his improbably forbearing wife contend with craziness and child-rearing in los angeles . \"\r\n",
      "+1 \" . . . with \"\" the bourne identity \"\" we return to the more traditional action genre . \"\r\n",
      "+1 \"you can watch , giggle and get an adrenaline boost without feeling like you've completely lowered your entertainment standards . \"\r\n",
      "+1 \"fun , flip and terribly hip bit of cinematic entertainment . \"\r\n",
      "+1 \"fisher has bared his soul and confronted his own shortcomings here in a way . . . that feels very human and very true to life . \"\r\n",
      "+1 \"while the plot follows a predictable connect-the-dots course . . . director john schultz colors the picture in some evocative shades . \"\r\n",
      "-1 \"the impact of the armenian genocide is diluted by too much stage business in the modern day . \"\r\n",
      "+1 \"thanks to haynes' absolute control of the film's mood , and buoyed by three terrific performances , far from heaven actually pulls off this stylistic juggling act . \"\r\n",
      "+1 \"we know the plot's a little crazy , but it held my interest from start to finish . \"\r\n",
      "-1 \"half of it is composed of snappy patter and pseudo-sophisticated cultural observations , while the remainder . . . would be more at home on a daytime television serial . \"\r\n",
      "+1 \"though it lacks the utter authority of a genre gem , there's a certain robustness to this engaging mix of love and bloodletting . \"\r\n",
      "-1 \"some writer dude , i think his name was , uh , michael zaidan , was supposed to have like written the screenplay or something , but , dude , the only thing that i ever saw that was written down were the zeroes on my paycheck . \"\r\n",
      "+1 \"the film is small in scope , yet perfectly formed . \"\r\n",
      "-1 \"the plot plummets into a comedy graveyard before janice comes racing to the rescue in the final reel . \"\r\n",
      "-1 \"it's the movie equivalent of a sweaty old guy in a rain coat shopping for cheap porn . \"\r\n",
      "+1 \"it is a likable story , told with competence . \"\r\n",
      "-1 \"in the second half of the film , frei's control loosens in direct proportion to the amount of screen time he gives nachtwey for self-analysis . \"\r\n",
      "-1 \"[morgan] , judd and franklin can't save the script , rooted in a novel by joseph finder , from some opportunism . \"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 ../work/sentiment.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. ストップワード\n",
    "英語のストップワードのリスト（ストップリスト）を適当に作成せよ．さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_word(word):\n",
    "    if not word:\n",
    "        return True\n",
    "    if len(word.rstrip()) <= 1:\n",
    "        return True\n",
    "    \n",
    "    STOP_WORD = set(\"\"\"\n",
    "    , . a an the at to on of for in by with above under\n",
    "    this that i you it he she they am are is was were \n",
    "    and but though although then as \n",
    "    \" ' - – ( ) *\n",
    "    \"\"\".lower().split())\n",
    "    return word.lower() in STOP_WORD\n",
    "\n",
    "assert is_stop_word('a')\n",
    "assert is_stop_word('the')\n",
    "assert is_stop_word('i')\n",
    "assert is_stop_word('I')\n",
    "assert is_stop_word('YOU')\n",
    "assert is_stop_word('\"')\n",
    "assert is_stop_word('*')\n",
    "assert is_stop_word('')\n",
    "assert is_stop_word('　')\n",
    "assert is_stop_word(None)\n",
    "assert is_stop_word('e')\n",
    "\n",
    "assert not is_stop_word('good')\n",
    "assert not is_stop_word('bad')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 素性抽出\n",
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元文\n",
      "['to portray modern women the way director davis has done is just unthinkable . ', \"kenneth branagh's energetic sweet-and-sour performance as a curmudgeonly british playwright grounds this overstuffed , erratic dramedy in which he and his improbably forbearing wife contend with craziness and child-rearing in los angeles . \"]\n",
      "\n",
      "マッピング\n",
      "['angel', 'branagh', 'branagh energet', 'british', 'british playwright', 'child-rear', 'child-rear los', 'contend', 'contend crazi', 'crazi', 'crazi child-rear', 'curmudgeon', 'curmudgeon british', 'davi', 'davi has', 'director', 'director davi', 'done', 'done just', 'dramedi', 'dramedi which', 'energet', 'energet sweet-and-sour', 'errat', 'errat dramedi', 'forbear', 'forbear wife', 'ground', 'ground overstuf', 'has', 'has done', 'his', 'his improb', 'improb', 'improb forbear', 'just', 'just unthink', 'kenneth', 'kenneth branagh', 'los', 'los angel', 'modern', 'modern women', 'overstuf', 'overstuf errat', 'perform', 'perform curmudgeon', 'playwright', 'playwright ground', 'portray', 'portray modern', 'sweet-and-sour', 'sweet-and-sour perform', 'unthink', 'way', 'way director', 'which', 'which his', 'wife', 'wife contend', 'women', 'women way']\n",
      "\n",
      "素性\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.22941573  0.22941573  0.22941573  0.22941573  0.22941573  0.22941573\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.22941573  0.22941573  0.          0.\n",
      "   0.          0.          0.22941573  0.22941573  0.          0.          0.\n",
      "   0.          0.22941573  0.22941573  0.          0.          0.          0.\n",
      "   0.          0.          0.22941573  0.22941573  0.          0.\n",
      "   0.22941573  0.22941573  0.22941573  0.          0.          0.          0.\n",
      "   0.22941573  0.22941573]\n",
      " [ 0.15249857  0.15249857  0.15249857  0.15249857  0.15249857  0.15249857\n",
      "   0.15249857  0.15249857  0.15249857  0.15249857  0.15249857  0.15249857\n",
      "   0.15249857  0.          0.          0.          0.          0.          0.\n",
      "   0.15249857  0.15249857  0.15249857  0.15249857  0.15249857  0.15249857\n",
      "   0.15249857  0.15249857  0.15249857  0.15249857  0.          0.\n",
      "   0.15249857  0.15249857  0.15249857  0.15249857  0.          0.\n",
      "   0.15249857  0.15249857  0.15249857  0.15249857  0.          0.\n",
      "   0.15249857  0.15249857  0.15249857  0.15249857  0.15249857  0.15249857\n",
      "   0.          0.          0.15249857  0.15249857  0.          0.          0.\n",
      "   0.15249857  0.15249857  0.15249857  0.15249857  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from stemming.porter2 import stem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def stem_tokenizer(sentence):\n",
    "    return [stem(word) for word in sentence.split(' ') if not is_stop_word(word)]\n",
    "\n",
    "def extract_feature(text, min_df=1):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=stem_tokenizer, ngram_range=(1,2), min_df=min_df)\n",
    "    vectorizer = vectorizer.fit(text)\n",
    "    vector = vectorizer.transform(text)\n",
    "    return vectorizer, vector.toarray()\n",
    "\n",
    "def knock_72():\n",
    "    df = pd.read_csv('../work/sentiment.txt', sep=' ')\n",
    "    text = df['Review'].tolist()[:2]\n",
    "    vectorizer, feature = extract_feature(text)\n",
    "    print('元文')\n",
    "    print(text)\n",
    "    print('\\nマッピング')\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print('\\n素性')\n",
    "    print(feature)\n",
    "\n",
    "knock_72()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 学習\n",
    "72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(train): 0.881280986199\n",
      "Accuracy(test): 0.774304470147\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def learn_by_logistic_regression(model_output_directory):\n",
    "    df = pd.read_csv('../work/sentiment.txt', sep=' ')\n",
    "    X = df['Review'].tolist()\n",
    "    y = df['Sentiment'].tolist()\n",
    "    vectorizer, feature = extract_feature(X, min_df=5)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature, y, test_size=0.3, shuffle=False)\n",
    "    \n",
    "    lr = LogisticRegression(solver='liblinear', random_state=0)\n",
    "    model = lr.fit(X_train, y_train)\n",
    "    print('Accuracy(train): ' + str(lr.score(X_train, y_train)))\n",
    "    print('Accuracy(test): ' + str(lr.score(X_test, y_test)))\n",
    "    \n",
    "    if not os.path.exists(model_output_directory):\n",
    "        os.makedirs(model_output_directory)\n",
    "    joblib.dump(model, model_output_directory + '/model.pkl')\n",
    "    joblib.dump(vectorizer, model_output_directory + '/vectorizer.pkl')\n",
    "\n",
    "learn_by_logistic_regression('../work/knock_73')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 予測\n",
    "73で学習したロジスティック回帰モデルを用い，与えられた文の極性ラベル（正例なら\"+1\"，負例なら\"-1\"）と，その予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is very funny -> +1 (0.737271911721)\n",
      "A story of this movie is excellent -> +1 (0.594921654769)\n",
      "This movie is too bad -> -1 (0.950025256101)\n",
      "This movie is terrible -> -1 (0.769327015675)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def predict_by_logistic_regression(model_directory, text_array):\n",
    "    lr = joblib.load(model_directory + '/model.pkl')\n",
    "    vectorizer = joblib.load(model_directory + '/vectorizer.pkl')\n",
    "    feature = vectorizer.transform(text_array).toarray()\n",
    "    predicted_labels = lr.predict(feature)\n",
    "    predicted_probabilities = lr.predict_proba(feature)\n",
    "    label_probability_pair = []\n",
    "    for i, predicted_label in enumerate(predicted_labels):\n",
    "        if predicted_label == 1:\n",
    "            label_probability_pair.append(('+1', predicted_probabilities[i][1]))\n",
    "        else:\n",
    "            label_probability_pair.append(('-1', predicted_probabilities[i][0]))\n",
    "    return label_probability_pair\n",
    "\n",
    "reviews = ['This movie is very funny', 'A story of this movie is excellent', 'This movie is too bad', 'This movie is terrible']\n",
    "label_probability_pair = predict_by_logistic_regression('../work/knock_73', reviews)\n",
    "for i, (label, probability) in enumerate(label_probability_pair):\n",
    "    print(reviews[i] + ' -> ' + label +  ' (' + str(probability) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. 素性の重み\n",
    "73で学習したロジスティック回帰モデルの中で，重みの高い素性トップ10と，重みの低い素性トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重みの高い素性トップ10\n",
      "[(2.8589637435772173, 'beauti'),\n",
      " (2.3956275116777275, 'funni'),\n",
      " (2.1508983631746128, 'still'),\n",
      " (2.10545333197128, 'enjoy'),\n",
      " (2.0909607897078031, 'perform'),\n",
      " (2.0803539894407517, 'flaw'),\n",
      " (2.075159838858827, 'delight'),\n",
      " (2.0336450108457118, 'power'),\n",
      " (2.0316332176758998, 'fun'),\n",
      " (1.9709466052059059, 'wonder')]\n",
      "\n",
      "重みの低い素性トップ10\n",
      "[(-3.6672470168602902, 'too'),\n",
      " (-3.1052604875431915, 'bad'),\n",
      " (-3.0348569442901798, 'dull'),\n",
      " (-3.0066066797164042, 'bore'),\n",
      " (-2.5026942197892539, 'lack'),\n",
      " (-2.3964732569463347, 'no'),\n",
      " (-2.1120580536399562, 'onli'),\n",
      " (-2.0819031773426446, 'wast'),\n",
      " (-2.0143804073628599, 'fail'),\n",
      " (-1.9862539662836505, 'worst')]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "def knock_75():\n",
    "    lr = joblib.load('../work/knock_73/model.pkl')\n",
    "    vectorizer = joblib.load('../work/knock_73/vectorizer.pkl')\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    weights = lr.coef_[0]\n",
    "    wieght_words_pair = list(zip(weights, feature_names))\n",
    "    sorted_weight_words_pair = sorted(wieght_words_pair, key=lambda x:x[0], reverse=True)\n",
    "\n",
    "    print('重みの高い素性トップ10')\n",
    "    pprint.pprint(sorted_weight_words_pair[:10])\n",
    "    print('\\n重みの低い素性トップ10')\n",
    "    pprint.pprint(sorted_weight_words_pair[:-11:-1])\n",
    "    \n",
    "knock_75()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. ラベル付け\n",
    "学習データに対してロジスティック回帰モデルを適用し，正解のラベル，予測されたラベル，予測確率をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def knock_76():\n",
    "    df = pd.read_csv('../work/sentiment.txt', sep=' ')\n",
    "    X = df['Review'].tolist()\n",
    "    y = df['Sentiment'].tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    label_probability_pair = predict_by_logistic_regression('../work/knock_73', X_train)\n",
    "    df = pd.concat([pd.DataFrame(y_train), pd.DataFrame(label_probability_pair)], axis=1)\n",
    "    df.columns = ['answer', 'predicted', 'pred_prob']\n",
    "    df['predicted'] = df['predicted'].astype(np.int64)\n",
    "    df.to_csv('../work/knock_73/result_of_train.tsv', sep='\\t', index=False)\n",
    "    \n",
    "knock_76()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\tpredicted\tpred_prob\r\n",
      "-1\t1\t0.6405216335704899\r\n",
      "-1\t1\t0.5336469161529501\r\n",
      "1\t1\t0.684957023248036\r\n",
      "1\t-1\t0.5379757077047782\r\n",
      "1\t1\t0.6998070442209906\r\n",
      "1\t1\t0.7762966417126509\r\n",
      "1\t-1\t0.5032244068733721\r\n",
      "-1\t-1\t0.7804381116937051\r\n",
      "1\t1\t0.847528632471801\r\n",
      "1\t-1\t0.5457859067882957\r\n",
      "-1\t-1\t0.6683289208681973\r\n",
      "1\t1\t0.6103842420868052\r\n",
      "-1\t-1\t0.9169552987061634\r\n",
      "1\t1\t0.868322305211289\r\n",
      "-1\t-1\t0.6917457059983712\r\n",
      "-1\t-1\t0.7182561700192462\r\n",
      "1\t1\t0.6450939087689525\r\n",
      "-1\t-1\t0.523991666110773\r\n",
      "-1\t-1\t0.7461548301598886\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 ../work/knock_73/result_of_train.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. 正解率の計測\n",
    "76の出力を受け取り，予測の正解率，正例に関する適合率，再現率，F1スコアを求めるプログラムを作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:\t0.884605158203\n",
      "recall:\t0.880391637999\n",
      "f1:\t0.8824933687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def knock_77():\n",
    "    df = pd.read_csv('../work/knock_73/result_of_train.tsv', sep='\\t')\n",
    "    print('precision:\\t' + str(precision_score(df['answer'], df['predicted'])))\n",
    "    print('recall:\\t' + str(recall_score(df['answer'], df['predicted'])))\n",
    "    print('f1:\\t' + str(f1_score(df['answer'], df['predicted'])))\n",
    "\n",
    "knock_77()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. 5分割交差検定\n",
    "76-77の実験では，学習に用いた事例を評価にも用いたため，正当な評価とは言えない．すなわち，分類器が訓練事例を丸暗記する際の性能を評価しており，モデルの汎化性能を測定していない．そこで，5分割交差検定により，極性分類の正解率，適合率，再現率，F1スコアを求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch\n",
      "Best score:\t0.760016079325\n",
      "Best parameter:\t{'C': 1, 'solver': 'newton-cg'}\n",
      "\n",
      "5分割交差検定\n",
      "{'fit_time': array([ 5.02494168,  5.01517606,  5.04694104,  5.0105083 ,  5.08638406]),\n",
      " 'score_time': array([ 0.05747604,  0.05380106,  0.05505228,  0.05360866,  0.05328178]),\n",
      " 'test_a': array([ 0.79240862,  0.76829268,  0.75469043,  0.76454034,  0.77861163]),\n",
      " 'test_f': array([ 0.79133302,  0.76785714,  0.75434476,  0.76476101,  0.77609108]),\n",
      " 'test_p': array([ 0.79545455,  0.7693032 ,  0.75540922,  0.76404494,  0.78502879]),\n",
      " 'test_r': array([ 0.78725398,  0.76641651,  0.7532833 ,  0.76547842,  0.7673546 ]),\n",
      " 'train_a': array([ 0.87664165,  0.87596717,  0.87620164,  0.87713951,  0.87549824]),\n",
      " 'train_f': array([ 0.87635167,  0.87555869,  0.87541293,  0.87647336,  0.87502942]),\n",
      " 'train_p': array([ 0.87841659,  0.87845173,  0.88102588,  0.88125148,  0.87833688]),\n",
      " 'train_r': array([ 0.87429644,  0.87268464,  0.86987104,  0.87174678,  0.87174678])}\n",
      "\n",
      "Average of accuracy:\t0.7717087413466593\n",
      "Average of precision:\t0.7738481401518558\n",
      "Average of recall:\t0.767957363230182\n",
      "Average of f1:\t0.7708774037492988\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def knock_78():\n",
    "    params = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'liblinear']\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv('../work/sentiment.txt', sep=' ')\n",
    "    X = df['Review'].tolist()\n",
    "    y = df['Sentiment'].tolist()\n",
    "    vectorizer, feature = extract_feature(X, min_df=5)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "    lr = LogisticRegression(random_state=0)\n",
    "    gridsearch_lr = GridSearchCV(lr, param_grid=params, n_jobs=-1, cv=3)\n",
    "    gridsearch_lr.fit(X_train, y_train)\n",
    "    print('GridSearch')\n",
    "    print('Best score:\\t' + str(gridsearch_lr.best_score_))\n",
    "    print('Best parameter:\\t' + str(gridsearch_lr.best_params_))\n",
    "    \n",
    "    scoring = {\n",
    "        'a': 'accuracy',\n",
    "        'p': 'precision',\n",
    "        'r': 'recall',\n",
    "        'f': 'f1'\n",
    "    }\n",
    "    best_estimator = gridsearch_lr.best_estimator_ \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=False, random_state=0)\n",
    "    scores = cross_validate(best_estimator, feature, y, cv=kfold, scoring=scoring)\n",
    "    print('\\n5分割交差検定')\n",
    "    pprint.pprint(scores)\n",
    "    print('\\nAverage of accuracy:\\t{}'.format(np.mean(scores['test_a'])))\n",
    "    print('Average of precision:\\t{}'.format(np.mean(scores['test_p'])))\n",
    "    print('Average of recall:\\t{}'.format(np.mean(scores['test_r'])))\n",
    "    print('Average of f1:\\t{}'.format(np.mean(scores['test_f'])))\n",
    "    \n",
    "    model_output_directory = '../work/knock_78'\n",
    "    if not os.path.exists(model_output_directory):\n",
    "        os.makedirs(model_output_directory)\n",
    "    joblib.dump(best_estimator, model_output_directory + '/model.pkl')\n",
    "    joblib.dump(vectorizer, model_output_directory + '/vectorizer.pkl')\n",
    "\n",
    "knock_78()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. 適合率-再現率グラフの描画\n",
    "ロジスティック回帰モデルの分類の閾値を変化させることで，適合率-再現率グラフを描画せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKEREJJCCQkREAv0PMpMqNuq+DVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77c7207c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def knock_79():\n",
    "    df = pd.read_csv('../work/sentiment.txt', sep=' ')\n",
    "    X = df['Review'].tolist()\n",
    "    y = df['Sentiment'].tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    \n",
    "    lr = joblib.load('../work/knock_78/model.pkl')\n",
    "    vectorizer = joblib.load('../work/knock_78/vectorizer.pkl')\n",
    "    feature = vectorizer.transform(X_test).toarray()\n",
    "    predicted_probabilities = lr.predict_proba(feature)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, predicted_probabilities[:,1])\n",
    "    \n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('recall')\n",
    "    plt.ylabel('precision')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()\n",
    "    \n",
    "knock_79()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
