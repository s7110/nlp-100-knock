{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法 (I)\n",
    "enwiki-20150112-400-r10-105752.txt.bz2は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．\n",
    "\n",
    "なお，問題83を素直に実装すると，大量（約7GB）の主記憶が必要になる． メモリが不足する場合は，処理を工夫するか，1/100サンプリングのコーパスenwiki-20150112-400-r100-10576.txt.bz2を用いよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. コーパスの整形\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "\n",
    "- トークンの先頭と末尾に出現する次の文字を削除: .,!?;:()[]'\"\n",
    "- 空文字列となったトークンは削除\n",
    "\n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\r\n",
      "\r\n",
      "Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions, but that several authors have defined as more specific institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary, or harmful. While anti-statism is central, anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including, but not limited to, the state system.\r\n",
      "As a subtle and anti-dogmatic philosophy, anarchism draws on many currents of thought and strategy. Anarchism does not offer a fixed body of doctrine from a single particular world view, instead fluxing and flowing as a philosophy. There are many types and traditions of anarchism, not all of which are mutually exclusive. Anarchist schools of thought can differ fundamentally, supporting anything from extreme individualism to complete collectivism. Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications. Anarchism is usually considered a radical left-wing ideology, and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism, collectivism, syndicalism, mutualism, or participatory economics.\r\n",
      "The central tendency of anarchism as a social movement has been represented by anarcho-communism and anarcho-syndicalism, with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organisations. Many anarchists oppose all forms of aggression, supporting self-defense or non-violence (anarcho-pacifism), while others have supported the use of some coercive measures, including violent revolution and propaganda of the deed as means to achieve anarchist ends.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ../data/enwiki-20150112-400-r100-10576.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    for token in line.split():\n",
    "        yield token.strip('.,!?;:()[]\\'\"')\n",
    "\n",
    "def knock_80():\n",
    "    with open('../data/enwiki-20150112-400-r100-10576.txt') as f_in:\n",
    "        with open('../work/enwiki-20150112-400-r100-10576-tokenized.txt', 'w') as f_out:\n",
    "            for line in f_in:\n",
    "                tokens = [cleaned_token for cleaned_token in tokenize(line) if cleaned_token]\n",
    "                if len(tokens) > 0:\n",
    "                    tokenized_sentence = ' '.join(tokens)\n",
    "                    f_out.write(tokenized_sentence + '\\n')\n",
    "\n",
    "knock_80()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\r\n",
      "Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions but that several authors have defined as more specific institutions based on non-hierarchical free associations Anarchism holds the state to be undesirable unnecessary or harmful While anti-statism is central anarchism entails opposing authority or hierarchical organisation in the conduct of human relations including but not limited to the state system\r\n",
      "As a subtle and anti-dogmatic philosophy anarchism draws on many currents of thought and strategy Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy There are many types and traditions of anarchism not all of which are mutually exclusive Anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications Anarchism is usually considered a radical left-wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics\r\n",
      "The central tendency of anarchism as a social movement has been represented by anarcho-communism and anarcho-syndicalism with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organisations Many anarchists oppose all forms of aggression supporting self-defense or non-violence anarcho-pacifism while others have supported the use of some coercive measures including violent revolution and propaganda of the deed as means to achieve anarchist ends\r\n",
      "Etymology and terminology\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ../work/enwiki-20150112-400-r100-10576-tokenized.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. 複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan\r\n",
      "Albania\r\n",
      "Algeria\r\n",
      "Andorra\r\n",
      "Angola\r\n",
      "Antigua & Deps\r\n",
      "Argentina\r\n",
      "Armenia\r\n",
      "Australia\r\n",
      "Austria\r\n",
      "Azerbaijan\r\n",
      "Bahamas\r\n",
      "Bahrain\r\n",
      "Bangladesh\r\n",
      "Barbados\r\n",
      "Belarus\r\n",
      "Belgium\r\n",
      "Belize\r\n",
      "Benin\r\n",
      "Bhutan\r\n"
     ]
    }
   ],
   "source": [
    "# 国名リストは以下のものを使用\n",
    "# https://gist.github.com/kalinchernev/486393efcca01623b18d\n",
    "!head -n 20 ../data/countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "def read_compound_name_countries():\n",
    "    with open('../data/countries') as f:\n",
    "        compound_name_countries = defaultdict(list)\n",
    "        for line in f:\n",
    "            country_name_parts = line.rstrip().split(' ')\n",
    "            if len(country_name_parts) > 1:\n",
    "                head_token = country_name_parts[0]\n",
    "                rest_token = country_name_parts[1:]\n",
    "                # keyはheadの単語、valueは続く単語列のlist\n",
    "                # 'United': [['Arab', 'Emirates'], ['Kingdom'], ['States']]\n",
    "                # 'St': [['Kitts', '&', 'Nevis'], ['Lucia']]\n",
    "                compound_name_countries[head_token].append(rest_token)\n",
    "        return compound_name_countries\n",
    "    \n",
    "def replace_compound_name_country_in_a_line(line):\n",
    "    replaced = []\n",
    "    compound_name_countries = read_compound_name_countries()\n",
    "    skip_counter = 0\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    for i, token in enumerate(tokens):\n",
    "        if skip_counter > 0:\n",
    "            # 置換した複合語の単語数だけ読み飛ばす\n",
    "            skip_counter -= 1\n",
    "            continue\n",
    "        if token in compound_name_countries:\n",
    "            rest_token_list = compound_name_countries[token]\n",
    "            for rest_token in rest_token_list:\n",
    "                if tokens[i+1: i+len(rest_token)+1] == rest_token:\n",
    "                    skip_counter = len(rest_token)\n",
    "                    replaced.append('_'.join(tokens[i: i+len(rest_token)+1]))\n",
    "        if skip_counter == 0:\n",
    "            replaced.append(token)\n",
    "    return ' '.join(replaced)\n",
    " \n",
    "def knock_81():\n",
    "    with open('../work/enwiki-20150112-400-r100-10576-tokenized.txt') as f_in:\n",
    "        with open('../work/enwiki-20150112-400-r100-10576-compound_replaced.txt', 'w') as f_out:\n",
    "            for line in f_in:\n",
    "                f_out.write(replace_compound_name_country_in_a_line(line) + '\\n')\n",
    "\n",
    "knock_81()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term anarchism is a compound word composed from the word anarchy and the suffix -ism themselves derived respectively from the Greek i.e anarchy from anarchos meaning one without rulers from the privative prefix ἀν- an- i.e without and archos i.e leader ruler cf archon or arkhē i.e authority sovereignty realm magistracy and the suffix or -ismos -isma from the verbal infinitive suffix -ίζειν -izein The first known use of this word was in 1539.\"Anarchist was the term adopted by Maximilien de Robespierre to attack those on the left whom he had used for his own ends during the French Revolution but was determined to get rid of though among these anarchists there were few who exhibited the social revolt characteristics of later anarchists There would be many revolutionaries of the early nineteenth century who contributed to the anarchist doctrines of the next generation such as William Godwin and Wilhelm Weitling but they did not use the word anarchist or anarchism in describing themselves or their beliefs Pierre-Joseph Proudhon was the first political philosopher to call himself an anarchist marking the formal birth of anarchism in the mid-nineteenth century Since the 1890s from France the term libertarianism has often been used as a synonym for anarchism and was used almost exclusively in this sense until the 1950s in the United_States its use as a synonym is still common outside the United_States On the other hand some use libertarianism to refer to individualistic free-market philosophy only referring to free-market anarchism as libertarian anarchism\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ../work/enwiki-20150112-400-r100-10576-compound_replaced.txt | grep United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The British Museum's Oceanic collections originate from the vast area of the Pacific Ocean stretching from Papua_New_Guinea to Easter Island from New_Zealand to Hawaii The three main anthropological groups represented in the collection are Polynesia Melanesia and Micronesia – Aboriginal art from Australia is considered separately in its own right Metal working was not indigenous to Oceania before Europeans arrived so many of the artefacts from the collection are made from stone shell bone and bamboo The British Museum is fortunate in having some of the earliest objects made in Oceania in its collections many of which were collected by members of Cook's and Vancouver's expeditions before Western culture significantly impacted on local practices and ways of thinking The Wilson cabinet of curiosities from Palau is another example of pre-contact ware A particularly important group of objects was purchased from the London Missionary Society in 1911 that includes the unique statue of A'a from Rurutu in the Austral Islands the rare idol from Mangareva island and a figure of a god from the Cook Islands Another highlight is the huge Hawaiian statue of Kū-ka-ili-moku or god of war one of three extant in the world The Oceanic Collection is also famous for its large Easter Island statues Hoa Hakananai'a and Moai Hava\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1000 ../work/enwiki-20150112-400-r100-10576-compound_replaced.txt | grep Papua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語$t$に関して，単語$t$と文脈語$c$のペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "\n",
    "- ある単語$t$の前後$d$単語を文脈語$c$として抽出する（ただし，文脈語に単語tそのものは含まない）\n",
    "- 単語$t$を選ぶ度に，文脈幅$d$は${1,2,3,4,5}$の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def knock_82():\n",
    "    with open('../work/enwiki-20150112-400-r100-10576-compound_replaced.txt') as f_in:\n",
    "        with open('../work/enwiki-20150112-400-r100-10576-collocation.txt', 'w') as f_out:\n",
    "            random.seed(0)\n",
    "            for line in f_in:\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                for i, token in enumerate(tokens):\n",
    "                    context_words = []\n",
    "                    d = random.randint(1, 5)\n",
    "                    context_words += tokens[max(0, i-d): i]\n",
    "                    context_words += tokens[i+1: i+1+d]\n",
    "                    for context_word in context_words:\n",
    "                        f_out.write('{}\\t{}\\n'.format(token, context_word))\n",
    "\n",
    "knock_82()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tis\r\n",
      "Anarchism\ta\r\n",
      "Anarchism\tpolitical\r\n",
      "Anarchism\tphilosophy\r\n",
      "is\tAnarchism\r\n",
      "is\ta\r\n",
      "a\tAnarchism\r\n",
      "a\tis\r\n",
      "a\tpolitical\r\n",
      "a\tphilosophy\r\n",
      "a\tthat\r\n",
      "political\tAnarchism\r\n",
      "political\tis\r\n",
      "political\ta\r\n",
      "political\tphilosophy\r\n",
      "political\tthat\r\n",
      "political\tadvocates\r\n",
      "political\tstateless\r\n",
      "political\tsocieties\r\n",
      "philosophy\tAnarchism\r\n",
      "philosophy\tis\r\n",
      "philosophy\ta\r\n",
      "philosophy\tpolitical\r\n",
      "philosophy\tthat\r\n",
      "philosophy\tadvocates\r\n",
      "philosophy\tstateless\r\n",
      "philosophy\tsocieties\r\n",
      "that\tis\r\n",
      "that\ta\r\n",
      "that\tpolitical\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 30 ../work/enwiki-20150112-400-r100-10576-collocation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. 単語／文脈の頻度の計測\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "- $f(t,c)$: 単語$t$と文脈語$c$の共起回数\n",
    "- $f(t,\\ast)$: 単語$t$の出現回数\n",
    "- $f(\\ast,c)$: 文脈語$c$の出現回数\n",
    "- $N$: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68094461it [02:03, 552263.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 68094461\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def knock_83():\n",
    "    with open('../work/enwiki-20150112-400-r100-10576-collocation.txt', 'r') as f:\n",
    "        total_count = 0\n",
    "        t_counter = defaultdict(int)\n",
    "        c_counter = defaultdict(int)\n",
    "        tc_counter = defaultdict(int)\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            t, c = line.lower().rstrip().split('\\t')\n",
    "            total_count += 1\n",
    "            t_counter[t] += 1\n",
    "            c_counter[c] += 1\n",
    "            tc_counter[t + '\\t' + c] += 1\n",
    "        with open('../work/t_counter.pkl', 'wb') as f:\n",
    "            pickle.dump(t_counter, f)\n",
    "        with open('../work/c_counter.pkl', 'wb') as f:\n",
    "            pickle.dump(c_counter, f)\n",
    "        with open('../work/tc_counter.pkl', 'wb') as f:\n",
    "            pickle.dump(tc_counter, f)\n",
    "        print('N: ' + str(total_count))\n",
    "    \n",
    "knock_83()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語文脈行列の作成\n",
    "83の出力を利用し，単語文脈行列Xを作成せよ．ただし，行列Xの各要素$X_{tc}$は次のように定義する．\n",
    "\n",
    "- $ f(t,c) \\geq 10 $ ならば，$ X_{tc} = \\textrm{PPMI}(t,c) = \\max \\left\\{ \\log \\frac {N \\times f(t,c)}{f(t,\\ast) \\times f(\\ast,c)}, 0 \\right\\} $\n",
    "- $ f(t,c)<10 $ ならば，$ X_{tc} = 0 $\n",
    "\n",
    "ここで，$\\textrm{PPMI}(t,c)$はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列$X$の行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列$X$のほとんどの要素は$0$になるので，非$0$の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "import math\n",
    "\n",
    "N = 68094461\n",
    "\n",
    "def knock_84():\n",
    "    with open('../work/t_counter.pkl', 'rb') as f:\n",
    "        t_counter = pickle.load(f)\n",
    "    with open('../work/c_counter.pkl', 'rb') as f:\n",
    "        c_counter = pickle.load(f)\n",
    "    with open('../work/tc_counter.pkl', 'rb') as f:\n",
    "        tc_counter = pickle.load(f)\n",
    "\n",
    "    word_to_index = { t:i for i, t in enumerate(t_counter) }\n",
    "    x = lil_matrix((len(t_counter), len(c_counter)))\n",
    "    for tc, tc_count in tc_counter.items():\n",
    "        if tc_count < 10:\n",
    "            continue\n",
    "        t, c = tc.split('\\t')\n",
    "        ppmi = max(math.log((N * tc_count) / (t_counter[t] * c_counter[c])), 0)\n",
    "        t_index = word_to_index[t]\n",
    "        c_index = word_to_index[c]\n",
    "        x[t_index, c_index] = ppmi\n",
    "    \n",
    "    with open('../work/x_ppmi.pkl', 'wb') as f:\n",
    "        pickle.dump(x, f)\n",
    "    with open('../work/word_to_index.pkl', 'wb') as f:\n",
    "        pickle.dump(word_to_index, f)\n",
    "\n",
    "knock_84()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 主成分分析による次元圧縮\n",
    "84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD\n",
    "\n",
    "def knock_85():\n",
    "    with open('../work/x_ppmi.pkl', 'rb') as f:\n",
    "        x_ppmi = pickle.load(f)\n",
    "    \n",
    "    pca = TruncatedSVD(n_components=300)\n",
    "    word_vector = pca.fit_transform(x_ppmi)\n",
    "   \n",
    "    with open('../work/word_vector.pkl', 'wb') as f:\n",
    "        pickle.dump(word_vector, f)\n",
    "    \n",
    "knock_85()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 単語ベクトルの表示\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.18841405e+00 -1.16733813e+00  5.88192661e-01 -5.31552149e+00\n",
      " -2.73868465e+00 -5.14571637e-01 -3.51967080e+00 -3.11619359e-01\n",
      " -8.00750192e-02 -3.00999544e-02  5.35869599e+00  2.96782521e-01\n",
      "  5.28023090e-01 -2.07343719e+00  1.51945365e+00 -9.80887213e-02\n",
      " -4.94769541e-01 -1.37029820e+00 -2.93637891e-01  1.42807161e+00\n",
      "  1.42390976e+00  1.48054472e+00 -1.09191297e+00  1.55293497e-01\n",
      "  9.95203580e-01 -5.30881693e-01 -1.60740449e+00 -4.63751760e-01\n",
      "  2.45274238e+00 -2.49901007e-01  3.49011004e+00 -4.33954615e-02\n",
      " -1.44739058e-01  6.39001089e-01  6.63234649e-01 -2.53717827e-01\n",
      "  5.31502746e-01  6.19995565e-02  1.32794942e+00  9.08111345e-01\n",
      "  1.00661888e+00  7.42727174e-01 -2.93903376e-02  3.08937065e-01\n",
      "  2.74166276e-01 -1.30062537e+00  1.92870932e+00 -6.66868005e-01\n",
      " -6.44848929e-01  2.64025875e-01 -4.08787430e-01  5.10416120e-01\n",
      " -1.89578034e+00 -2.50026319e-01 -2.94490330e-01  4.53999684e-01\n",
      "  1.07606100e+00 -8.09314650e-01  1.87310839e-01 -9.76229163e-01\n",
      " -3.10846159e-01  1.97882992e-01 -1.01195610e+00  9.20347415e-01\n",
      "  5.85666193e-01  6.76085632e-01  2.86669358e+00 -1.82891476e+00\n",
      " -6.95527466e-02  1.05605341e+00 -7.90657152e-01  2.08874331e-02\n",
      " -9.36228156e-01 -9.89774249e-02 -9.23893363e-01  3.79035557e-01\n",
      " -1.57099757e+00  1.21141926e-01 -4.48532498e-03 -1.44734167e+00\n",
      " -1.31168420e-01 -5.13564238e-01  6.23984128e-01  3.32541928e-01\n",
      " -4.17625758e-01 -1.03740431e+00 -6.47751731e-01 -2.61451325e-01\n",
      " -5.36481019e-02  5.98071092e-01  5.20714203e-01  1.13213892e+00\n",
      "  5.58060802e-01  7.61663552e-01  9.28104192e-01  3.51906426e-01\n",
      "  4.17814704e-01  1.13586511e+00 -6.60768662e-02  2.72846705e-01\n",
      "  1.34023419e+00  1.84231305e+00 -1.34985534e+00 -5.80648465e-01\n",
      " -1.10217050e+00  1.02233138e+00  1.72307704e+00 -7.00243645e-01\n",
      " -1.08922051e+00 -1.11121569e-01  1.79281804e+00 -1.47372886e-01\n",
      "  1.73646517e+00 -4.07542868e-01 -1.40003289e+00  1.65805546e+00\n",
      "  2.63660483e-02  1.49498396e-01 -5.23119983e-01  7.01546162e-01\n",
      " -2.87717923e-01  7.11757506e-01 -8.57475102e-01 -5.89708278e-01\n",
      "  5.51494518e-01 -7.35164860e-01  6.64945730e-01 -6.73249721e-01\n",
      " -1.81733859e+00 -4.00518919e-01 -1.32398329e-01  1.60132112e+00\n",
      " -9.24327534e-01 -1.77806796e+00 -3.73940776e-01  8.94454858e-01\n",
      "  2.70359855e-01  1.26864796e-01  3.26970928e-01 -9.28776408e-02\n",
      "  1.34941845e+00 -3.98963069e-01 -2.27301241e-01 -1.12161163e+00\n",
      "  1.21912112e+00  4.75760731e-01  2.65967843e-01 -1.48563608e+00\n",
      "  1.07792292e-01  2.45915764e-02  4.60371669e-01  1.18564202e+00\n",
      " -1.42616332e+00 -4.99009223e-01  6.10973177e-01  1.89505836e+00\n",
      "  3.51488639e-01  7.79880441e-01 -5.79039059e-01 -3.93013357e-01\n",
      " -7.12701409e-01 -1.50759693e-01  9.57062207e-02 -1.25913740e+00\n",
      " -3.39297455e-01  2.61060232e-01 -6.55563401e-01  9.31122412e-01\n",
      "  8.87363399e-01 -1.72648332e+00 -3.64735715e-01  1.29799823e+00\n",
      " -1.77217385e+00 -6.52987605e-01  1.35289630e+00 -1.63296828e-01\n",
      "  1.49092916e+00 -5.13745160e-01 -1.68917548e+00  4.95201612e-01\n",
      "  9.59604213e-01  3.79468057e-02  2.93948356e-01 -8.77598420e-02\n",
      "  1.31360747e+00 -3.34591360e-01  3.61953722e-01 -6.87023990e-01\n",
      "  6.45361054e-01  9.88790574e-01 -4.10609532e-02 -1.48321116e-01\n",
      "  2.28263827e+00  1.47407359e-01  8.34201046e-01  3.73377552e-02\n",
      "  9.88722663e-01 -2.98894968e-01  9.41138559e-01 -2.65199583e-01\n",
      "  3.25323687e-01 -2.08238615e-01 -7.90815327e-01 -2.91632685e-01\n",
      "  1.43525425e-01 -3.36347240e-01 -4.49250612e-01  2.91846849e-01\n",
      " -2.85885060e-01 -6.48132465e-01  1.09490921e-01 -4.58904170e-01\n",
      " -3.18052368e-01  1.45959132e+00 -1.66372426e-01  1.49490360e+00\n",
      "  6.95338242e-02 -1.83030134e+00 -2.50363967e+00 -3.38175145e-01\n",
      " -3.45164664e-01 -8.42631798e-01  3.44963958e-01 -1.92862525e-01\n",
      " -1.73690308e+00 -8.72980560e-02  5.16371231e-01  4.78696529e-02\n",
      " -6.67748040e-01  7.10960445e-01 -3.07285294e-01  3.50276894e-02\n",
      "  1.24881409e-02  6.61527221e-01 -5.27337919e-01 -3.10335676e-01\n",
      "  5.04000263e-01 -5.68274408e-01 -1.00791038e+00  3.86592019e-02\n",
      " -1.03213069e-01  8.53859480e-02 -2.99551402e-01  1.08480929e+00\n",
      "  5.30908533e-01 -5.17349991e-01 -1.57452050e+00 -1.07467827e+00\n",
      " -6.13339859e-01 -6.80349631e-01 -9.20256452e-02 -3.93220694e-03\n",
      " -2.70978555e-01 -1.37836184e+00 -5.79560073e-01 -1.10882557e+00\n",
      " -2.53165615e-01 -1.68440916e-01 -2.42900352e-01 -5.14091882e-01\n",
      "  6.44083596e-01 -9.01363073e-01  1.18057872e+00 -7.89961305e-01\n",
      "  7.58990392e-01  3.85165726e-01  4.55755327e-01  8.17638592e-01\n",
      " -3.56850570e-01  3.10110104e-01  6.23256803e-01 -1.14803551e-01\n",
      " -6.05089590e-01 -7.84005176e-01  5.81318048e-01 -7.52769541e-01\n",
      " -1.42742871e-01  2.58185037e-01 -4.23051459e-01 -5.64473718e-01\n",
      "  4.01878942e-02  2.35278316e-01  1.15489510e-01  1.30551834e-01\n",
      "  4.12157923e-01  2.28798312e-01 -1.19086378e-01  5.67290111e-01\n",
      "  2.24481587e-01 -6.15240989e-01  2.33746109e-01  6.13592816e-01\n",
      "  5.27000123e-01  6.54856372e-01  9.32031931e-01 -2.42289400e-02\n",
      "  7.81869191e-01  2.93849316e-01  1.14431292e+00 -3.03908576e-01]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def knock_86():\n",
    "    with open('../work/word_vector.pkl', 'rb') as f:\n",
    "        word_vector = pickle.load(f)\n",
    "    with open('../work/word_to_index.pkl', 'rb') as f:\n",
    "        word_to_index = pickle.load(f)\n",
    "    print(word_vector[word_to_index['United_States'.lower()]])\n",
    "\n",
    "knock_86()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 単語の類似度\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829616260585497\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def knock_87():\n",
    "    with open('../work/word_vector.pkl', 'rb') as f:\n",
    "        word_vector = pickle.load(f)\n",
    "    with open('../work/word_to_index.pkl', 'rb') as f:\n",
    "        word_to_index = pickle.load(f)\n",
    "        \n",
    "    x = word_vector[word_to_index['United_States'.lower()]]\n",
    "    y = word_vector[word_to_index['U.S'.lower()]]\n",
    "    print(cosine_similarity([x],[y])[0][0])\n",
    "\n",
    "knock_87()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('scotland', 0.7315167033166496),\n",
      " ('australia', 0.6475282415285533),\n",
      " ('wales', 0.608772335595254),\n",
      " ('spain', 0.6068454168091277),\n",
      " ('france', 0.5842340500667358),\n",
      " ('italy', 0.5777101779948076),\n",
      " ('ireland', 0.5754315996341104),\n",
      " ('germany', 0.566933012313069),\n",
      " ('united_kingdom', 0.5372928727506192),\n",
      " ('japan', 0.52293520211957)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pprint\n",
    "\n",
    "def get_similar_words_with_vector(target_vector, top_n, word_vectors, word_to_index, exclude_words=None):\n",
    "    if not exclude_words:\n",
    "        exclude_words = ()\n",
    "    cosine_similarities = { word:cosine_similarity([target_vector],[word_vectors[index]])[0][0] for word,index in word_to_index.items() if word != exclude_words}\n",
    "    return sorted(cosine_similarities.items(), key=lambda x:x[1], reverse=True)[0:top_n]\n",
    "    \n",
    "def get_similar_words(target_word, top_n):\n",
    "    with open('../work/word_vector.pkl', 'rb') as f:\n",
    "        word_vectors = pickle.load(f)\n",
    "    with open('../work/word_to_index.pkl', 'rb') as f:\n",
    "        word_to_index = pickle.load(f)\n",
    "    target_vector = word_vectors[word_to_index[target_word.lower()]]\n",
    "    return get_similar_words_with_vector(target_vector, top_n, word_vectors, word_to_index, (target_word.lower()))\n",
    "\n",
    "def knock_88():\n",
    "    pprint.pprint(get_similar_words(\"England\", 10))\n",
    "    \n",
    "knock_88()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spain', 0.882086888194791),\n",
      " ('sweden', 0.8430648590532241),\n",
      " ('italy', 0.8199000557649473),\n",
      " ('germany', 0.8155642697586805),\n",
      " ('austria', 0.8099618510512393),\n",
      " ('denmark', 0.7961119683916021),\n",
      " ('netherlands', 0.7801191670399344),\n",
      " ('belgium', 0.7740889027588628),\n",
      " ('finland', 0.7582261651974795),\n",
      " ('télévisions', 0.7465486148512578)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pprint\n",
    "\n",
    "def knock_89():\n",
    "    with open('../work/word_vector.pkl', 'rb') as f:\n",
    "        word_vectors = pickle.load(f)\n",
    "    with open('../work/word_to_index.pkl', 'rb') as f:\n",
    "        word_to_index = pickle.load(f)\n",
    "    spain_vector = word_vectors[word_to_index['Spain'.lower()]]\n",
    "    madrid_vector = word_vectors[word_to_index['Madrid'.lower()]]\n",
    "    athens_vector = word_vectors[word_to_index['Athens'.lower()]]\n",
    "    target_vector = spain_vector - madrid_vector + athens_vector\n",
    "    pprint.pprint(get_similar_words_with_vector(target_vector, 10, word_vectors, word_to_index))\n",
    "    \n",
    "knock_89()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
